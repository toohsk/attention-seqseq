{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer import Chain, Variable, cuda, functions, links, optimizer, optimizers, serializers\n",
    "import datetime\n",
    "from filer3 import Filer\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from logging import getLogger, StreamHandler, INFO, FileHandler, Formatter\n",
    "\n",
    "LOG_FILE='log/att_s2s_chainer.log'\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "formatter = Formatter('[%(asctime)s] %(message)s')\n",
    "stream_handler = StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = FileHandler(LOG_FILE, 'a+')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.setLevel(INFO)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(\"Set constants.\")\n",
    "\n",
    "EOS = '<eos>'\n",
    "NIL = '<nil>'\n",
    "UNKNOWN = '<unkn>'\n",
    "\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 150\n",
    "BATCH_SIZE = 100\n",
    "EPOCH_NUM = 50\n",
    "\n",
    "FLAG_GPU = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_Encoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        クラスの初期化\n",
    "        :pram vocab_size: 語彙数\n",
    "        :pram embed_size: 中間ベクトルのサイズ（デフォルト200）\n",
    "        :pram hidden_size: 隠れ層のサイズ\n",
    "        \"\"\"\n",
    "        super(LSTM_Encoder, self).__init__(\n",
    "            xe = links.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            eh = links.Linear(embed_size, 4 * hidden_size),\n",
    "            hh = links.Linear(hidden_size, 4 * hidden_size)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, c, h):\n",
    "        \"\"\"\n",
    "        \n",
    "        :pram x: one-hotな単語\n",
    "        :pram c: 内部メモリ\n",
    "        :pram h: 隠れ層\n",
    "        :return: 次の内部メモリ， 次のかくれ層\n",
    "        \"\"\"\n",
    "        e = functions.tanh(self.xe(x))\n",
    "        return functions.lstm(c, self.eh(e) + self.hh(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM_Decoder(Chain):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        クラスの初期化\n",
    "        :param vocab_size: 語彙サイズ\n",
    "        :param embed_size: 単語ベクトルのサイズ\n",
    "        :param hidden_size: 中間ベクトルのサイズ\n",
    "        \"\"\"\n",
    "        super(LSTM_Decoder, self).__init__(\n",
    "            ye = links.EmbedID(vocab_size, embed_size, ignore_label=-1),\n",
    "            eh = links.Linear(embed_size, 4 * hidden_size),\n",
    "            hh = links.Linear(hidden_size, 4 * hidden_size),\n",
    "            he = links.Linear(hidden_size, embed_size),\n",
    "            ey = links.Linear(embed_size, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, y, c, h):\n",
    "        \"\"\"\n",
    "        \n",
    "        :pram y: one-hotな単語\n",
    "        :pram c: 内部メモリ\n",
    "        :pram h: 隠れ層\n",
    "        :return: 予測単語，　次の内部メモリ，　次の隠れそう\n",
    "        \"\"\"        \n",
    "        e = functions.tanh(self.ye(y))\n",
    "        c, h = functions.lstm(c, self.eh(e) + self.hh(h))\n",
    "        t = self.ey(functions.tanh(self.he(h)))\n",
    "        return t, c, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(Chain):\n",
    "    def __init__(self, in_vocab_size, out_vocab_size, embed_size, hidden_size, batch_size, flag_gpu = False):\n",
    "        \"\"\"\n",
    "        Seq2Seq の初期化\n",
    "        :parm in_vocab_size: 入力層で扱う語彙数\n",
    "        :parm out_vocab_size: 出力層で扱う語彙数\n",
    "        :parm　embed_size: 特徴ベクトルのサイズ\n",
    "        :parm hidden_size: 中間層のサイズ\n",
    "        :parm batch_size: ミニバッチのサイズ\n",
    "        :parm flag_gpu: GPUを使うかどうか\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__(\n",
    "            # Encoder のインスタンス化\n",
    "            encoder = LSTM_Encoder(\n",
    "                vocab_size=in_vocab_size,\n",
    "                embed_size=embed_size,\n",
    "                hidden_size=hidden_size\n",
    "            ),\n",
    "            # Decoder のインスタンス化\n",
    "            decoder = LSTM_Decoder(\n",
    "                vocab_size=out_vocab_size,\n",
    "                embed_size=embed_size,\n",
    "                hidden_size=hidden_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_vocab_size = in_vocab_size\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if flag_gpu:\n",
    "            self.ARR = cuda.cupy\n",
    "        else:\n",
    "            self.ARR = np\n",
    "            \n",
    "    def encode(self, words):\n",
    "        \"\"\"\n",
    "        Encoderを計算する部分\n",
    "        :param words: 単語のリスト\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 内部メモリ，中間ベクトルの初期化\n",
    "        c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        # エンコーダに単語を順に読み込む\n",
    "        for w in words:\n",
    "            c, h = self.encoder(w, c, h)\n",
    "            \n",
    "        # 計算した中間ベクトルをデコーダーに引き継ぐためにインスタンス変数にする\n",
    "        self.h = h\n",
    "        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "    def decode(self, w):\n",
    "        \"\"\"\n",
    "        デコーダーを計算する\n",
    "        :param w: 単語\n",
    "        :return: 単語数サイズのベクトルを出力\n",
    "        \"\"\"\n",
    "        t, self.c, self.h = self.decoder(w, self.c, self.h)\n",
    "        return t\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        中間ベクトル，　内部メモリ，　勾配の初期化\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.h = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        self.c = Variable(self.ARR.zeros((self.batch_size, self.hidden_size), dtype='float32'))\n",
    "        \n",
    "        self.zerograds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(enc_words, dec_words, model, ARR):\n",
    "    \"\"\"\n",
    "    順伝播の計算\n",
    "    :param enc_words: 入力文の単語を記録したリスト\n",
    "    :param dec_words: 出力文の単語を記録したリスト\n",
    "    :param model: Seq2Seqのインスタンス\n",
    "    :param ARR: cuda.cupyかnumpyか\n",
    "    :return: 計算した損失の合計\n",
    "    \"\"\"\n",
    "    batch_size = len(enc_words[0])\n",
    "    model.reset()\n",
    "    enc_words = [Variable(ARR.array(row, dtype='int32')) for row in enc_words]\n",
    "    model.encode(enc_words)\n",
    "    loss = Variable(ARR.zeros((), dtype='float32'))\n",
    "    # <eos> をデコーダに読み込ませる\n",
    "    t = Variable(ARR.array([0 for _ in range(batch_size)], dtype='int32'))\n",
    "    \n",
    "    # デコーダの計算\n",
    "    for w in dec_words:\n",
    "        y = model.decode(t)\n",
    "        t = Variable(ARR.array(w, dtype='int32'))\n",
    "        loss += functions.softmax_cross_entropy(y, t)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def forward_test(enc_words, model, ARR):\n",
    "    ret = []\n",
    "    model.reset()\n",
    "    enc_words = [Variable(ARR.array(row, dtype='int32')) for row in enc_words]\n",
    "    model.encode(enc_words)\n",
    "    t = Variable(ARR.array([0], dtype='int32'))\n",
    "    counter = 0\n",
    "    while counter < 50:\n",
    "        y = model.decode(t)\n",
    "        label = y.data.argmax()\n",
    "        ret.append(label)\n",
    "        t = Variable(ARR.array([label], dtype='int32'))\n",
    "        counter += 1\n",
    "        if label == 1:\n",
    "            counter = 50\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_minibatch(src_lines, entity_lines, minibatch):\n",
    "    # enc_words の作成\n",
    "    enc_words = [src_lines[idx] for idx in minibatch]\n",
    "    enc_max = np.max([len(row) for row in enc_words])\n",
    "    enc_words = np.array([[-1]*(enc_max - len(row)) + row for row in enc_words], dtype='int32')\n",
    "    enc_words = enc_words.T\n",
    "    \n",
    "    # dec_words の作成\n",
    "    dec_words = [entity_lines[idx] for idx in minibatch]\n",
    "    dec_max = np.max([len(row) for row in dec_words])\n",
    "    dec_words = np.array([[-1]*(dec_max - len(row)) + row for row in dec_words], dtype='int32')\n",
    "    dec_words = dec_words.T\n",
    "    \n",
    "    return enc_words, dec_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # 辞書の読み込み\n",
    "#     word_to_id = Filer.readdump(DICT_PATH)\n",
    "    src_lines, entity_lines, src_vocab, entity_vocab, id2wd, max_output_length = read_data_sources()\n",
    "    \n",
    "    # 語彙数\n",
    "    in_vocab_size = len(src_vocab)\n",
    "    out_vocab_size = len(entity_vocab)\n",
    "    \n",
    "    # モデルのインスタンス化\n",
    "    model = Seq2Seq(\n",
    "        in_vocab_size=in_vocab_size,\n",
    "        out_vocab_size=out_vocab_size,\n",
    "        embed_size=EMBED_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        flag_gpu = FLAG_GPU\n",
    "    )\n",
    "    model.reset()\n",
    "    \n",
    "    # GPUのセット\n",
    "    if FLAG_GPU:\n",
    "        ARR = cuda.cupy\n",
    "        cuda.get_device(0).use()\n",
    "        model.to_gpu(0)\n",
    "    else:\n",
    "        ARR = np\n",
    "\n",
    "    # 学習開始\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        # エポックごとにoptimizerの初期化\n",
    "        opt = optimizers.Adam()\n",
    "        opt.setup(model)\n",
    "        opt.add_hook(optimizer.GradientClipping(5))\n",
    "        \n",
    "        data = list(range(len(src_lines)))\n",
    "        random.shuffle(data)\n",
    "        for num in range(len(data)//BATCH_SIZE):\n",
    "            minibatch = data[num*BATCH_SIZE: (num+1)*BATCH_SIZE]\n",
    "            # 読み込み用のデータ作成\n",
    "            enc_words, dec_words = make_minibatch(src_lines, entity_lines, minibatch)\n",
    "            # modelのリセット\n",
    "            model.reset()\n",
    "            # 順伝播\n",
    "            total_loss = forward(enc_words=enc_words,\n",
    "                                 dec_words=dec_words,\n",
    "                                 model=model,\n",
    "                                 ARR=ARR)\n",
    "            # 学習\n",
    "            logger.info(\"train loss: {0}\".format(total_loss))\n",
    "            total_loss.backward()\n",
    "            opt.update()\n",
    "            opt.zero_grads()\n",
    "\n",
    "        logger.info('Epoch %s 終了' % (epoch+1))\n",
    "        outputpath = './model/s2s_{0}_{1}_{2}_{3}'.format(EMBED_SIZE, HIDDEN_SIZE, BATCH_SIZE, epoch+1)\n",
    "        serializers.save_hdf5(outputpath, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def read_data_from_pickle(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def create_word_dic(lines):\n",
    "    word_dic = {}\n",
    "    id2wd = {}\n",
    "    for line in lines:\n",
    "        lt = line.split()\n",
    "        for w in lt:\n",
    "            if w not in word_dic:\n",
    "                vocab_size = len(word_dic)\n",
    "                word_dic[w] = vocab_size\n",
    "                id2wd[vocab_size] = w\n",
    "                \n",
    "    return word_dic, id2wd\n",
    "\n",
    "def lines2words(lines, word_dict):\n",
    "    token_lines = []\n",
    "    for line in lines:\n",
    "        token_id_list = []\n",
    "        for token in line.split():\n",
    "            token_id_list.append(word_dict[token])\n",
    "        token_lines.append(token_id_list)\n",
    "    return token_lines\n",
    "    \n",
    "def read_data_sources():\n",
    "    logger.info(\"Loading data.\")\n",
    "    src_lines = read_data_from_pickle('./data/seq2seq_wiki/src/src_list_1.pickle').decode('utf8').split('\\n')\n",
    "    # dest_lines = read_data_from_pickle('./data/seq2seq_wiki/dest_all/replace_entity_list_1.pickle').decode('utf8').split('\\n')\n",
    "    entity_lines = read_data_from_pickle('./data/seq2seq_wiki/dest_entity/entity_list_with_nil_1.pickle').decode('utf8').split('\\n')\n",
    "\n",
    "    src_vocab, _ = create_word_dic(src_lines)\n",
    "    src_vocab[EOS] = len(src_vocab)\n",
    "    pv = len(src_vocab)\n",
    "    \n",
    "    max_output_length = 0\n",
    "    for line in entity_lines:\n",
    "        le = line.split()\n",
    "        if len(le) > max_output_length:\n",
    "            max_output_length = len(le)\n",
    "    \n",
    "    entity_vocab, id2wd = create_word_dic(entity_lines)\n",
    "\n",
    "    if EOS not in entity_vocab:\n",
    "        logger.info(\"Add <eos> to dictionary.\")\n",
    "        vocab_size = len(entity_vocab)\n",
    "        entity_vocab[EOS] = vocab_size\n",
    "        id2wd[vocab_size] = EOS\n",
    "\n",
    "    if UNKNOWN not in entity_vocab:\n",
    "        logger.info(\"Add <unkn> to dictionary.\")\n",
    "        vocab_size = len(entity_vocab)\n",
    "        entity_vocab[UNKNOWN] = vocab_size\n",
    "        id2wd[vocab_size] = UNKNOWN\n",
    "\n",
    "    if NIL not in entity_vocab:\n",
    "        logger.info(\"Add <nil> to dictionary.\")\n",
    "        vocab_size = len(entity_vocab)\n",
    "        entity_vocab[NIL] = vocab_size\n",
    "        id2wd[vocab_size] = NIL\n",
    "\n",
    "    return lines2words(src_lines, src_vocab), lines2words(entity_lines, entity_vocab), src_vocab, entity_vocab, id2wd, max_output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username='Seq2Seq'\n",
    "\n",
    "logger.info('Start training')\n",
    "try:\n",
    "    train()\n",
    "except:\n",
    "    logger.error('Error')\n",
    "    raise\n",
    "\n",
    "logger.info('Finish training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def mk_ct(gh, ht):\n",
    "#     alp = []\n",
    "#     s = 0.0\n",
    "#     for i in range(len(gh)):\n",
    "#         s += np.exp(ht.dot(gh[i]))\n",
    "#     ct = np.zeros(100)\n",
    "#     for i in range(len(gh)):\n",
    "#         alpi = np.exp(ht.dot(gh[i]))/s\n",
    "#         ct += alpi * gh[i]\n",
    "#     ct = Variable(np.array([ct]).astype(np.float32))\n",
    "#     return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class ATT(chainer.Chain):\n",
    "#     def __init__(self, pv, ev, k):\n",
    "#         super(ATT, self).__init__(\n",
    "#             embedx = L.EmbedID(pv, k),\n",
    "#             embedy = L.EmbedID(ev, k),\n",
    "#             H = L.LSTM(k, k),\n",
    "#             Wc1 = L.Linear(k, k),\n",
    "#             Wc2 = L.Linear(k, k),\n",
    "#             W = L.Linear(k, ev),\n",
    "#         )\n",
    "        \n",
    "#     def __call__(self, pline, eline):\n",
    "#         gh = []\n",
    "#         for i in range(len(pline)):\n",
    "#             wid = src_vocab[pline[i]]\n",
    "#             x_k = self.embedx(Variable(np.array([wid], dtype=np.int32)))\n",
    "#             h = self.H(x_k)\n",
    "#             gh.append(np.copy(h.data[0]))\n",
    "            \n",
    "#         x_k = self.embedx(Variable(np.array([src_vocab[EOS]], dtype=np.int32)))\n",
    "#         tx = Variable(np.array([entity_vocab[eline[0]]], dtype=np.int32))\n",
    "#         h = self.H(x_k)\n",
    "#         ct = mk_ct(gh, h.data[0])\n",
    "#         h2 = F.tanh(self.Wc1(ct) + self.Wc2(h))\n",
    "#         accum_loss = F.softmax_cross_entropy(self.W(h2), tx)\n",
    "        \n",
    "#         for i in range(len(eline)):\n",
    "#             wid = entity_vocab[eline[i]]\n",
    "#             x_k = self.embedy(Variable(np.array([wid], dtype=np.int32)))\n",
    "#             next_wid = entity_vocab[EOS] if ( i == len(eline) - 1) else entity_vocab[eline[i+1]]\n",
    "#             tx = Variable(np.array([next_wid], dtype=np.int32))\n",
    "#             h = self.H(x_k)\n",
    "#             ct = mk_ct(gh, h.data)\n",
    "#             h2 = F.tanh(self.Wc1(ct) + self.Wc2(h))\n",
    "#             loss = F.softmax_cross_entropy(self.W(h2), tx)\n",
    "#             accum_loss += loss\n",
    "            \n",
    "#         return accum_loss\n",
    "    \n",
    "#     def reset_state(self):\n",
    "#         self.H.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demb = 100\n",
    "# model = ATT(pv, ev, demb)\n",
    "# optimizer = optimizers.Adam()\n",
    "# optimizer.setup(model)\n",
    "\n",
    "# n_epoch = 50\n",
    "\n",
    "# logger.info(\"line num: {}\", len(src_lines))\n",
    "# import datetime\n",
    "\n",
    "# for epoch in range(n_epoch):\n",
    "#     sum_loss = 0\n",
    "    \n",
    "#     for i in range(len(src_lines)-1):\n",
    "#         if i % 1000 == 0:\n",
    "#             logger.info(\"{0}: Epoch {1} - Lines {2}...\".format(datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\"), epoch, i))\n",
    "#             print(\"{0}: Epoch {1} - Lines {2}...\".format(datetime.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\"), epoch, i))\n",
    "#         pln = src_lines[i].split()\n",
    "#         plnr = pln[::-1]\n",
    "#         eln = entity_lines[i].split()\n",
    "#         if len(eln) == 0:\n",
    "#             eln = [NIL]\n",
    "# #         print(eln)\n",
    "#         model.reset_state()\n",
    "#         model.zerograds()\n",
    "#         loss = model(plnr, eln)\n",
    "#         sum_loss += loss.data\n",
    "#         loss.backward()\n",
    "#         loss.unchain_backward()\n",
    "#         optimizer.update()\n",
    "#     logger.info(\"{0} finished\".format(epoch))\n",
    "#     logger.info(\"train loss: {0}\".format(sum_loss))\n",
    "#     print(\"{0} finished\".format(epoch), flush=True)\n",
    "#     print(\"train loss: {0}\".format(sum_loss), flush=True)\n",
    "    \n",
    "#     if epoch == n_epoch-1:\n",
    "#         outfile = \"attention-\"+str(n_epoch)+\".model\"\n",
    "#         serializers.save_npz(outfile, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def mt(model, src_line, loop_limit=30):\n",
    "#     lt = src_line.split()[::-1]\n",
    "#     for i in range(len(lt)):\n",
    "#         wid = src_vocab[lt[i]]\n",
    "#         # print(lt[i])\n",
    "#         x_k = model.embedx(Variable(np.array([wid], dtype=np.int32)))\n",
    "#         h = model.H(x_k)\n",
    "#     x_k = model.embedx(Variable(np.array([src_vocab[EOS]], dtype=np.int32)))\n",
    "#     h = model.H(x_k)\n",
    "#     wid = np.argmax(F.softmax(model.W(h)).data[0])\n",
    "    \n",
    "#     src_ary = []\n",
    "#     if wid in id2wd:\n",
    "#         src_ary.append(id2wd[wid])\n",
    "#     else:\n",
    "#         src_ary.append(UNKNOWN)\n",
    "    \n",
    "#     logger.info(' / '.join(src_ary))\n",
    "# #     print(' / '.join(src_ary))\n",
    "    \n",
    "#     loop = 0\n",
    "#     mt_ary = []\n",
    "#     while (wid != entity_vocab[EOS]) and (loop <= loop_limit):\n",
    "#         x_k = model.embedy(Variable(np.array([wid], dtype=np.int32)))\n",
    "#         h = model.H(x_k)\n",
    "#         wid = np.argmax(F.softmax(model.W(h)).data[0])\n",
    "    \n",
    "#         if wid in id2wd:\n",
    "#             mt_ary.append(id2wd[wid])\n",
    "#         else:\n",
    "#             mt_ary.append(UNKNOWN)\n",
    "            \n",
    "#         loop += 1\n",
    "# #     print(' / '.join(mt_ary))\n",
    "#     logger.info(' / '.join(mt_ary))\n",
    "#     logger.info(' ----------------- ')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"max loop limit: {}\".format(max_output_length))\n",
    "# logger.info(\"max loop limit: {}\".format(max_output_length))\n",
    "# for line in src_lines:\n",
    "# #     print(line)\n",
    "#     logger.info(line)\n",
    "#     mt(model, line, loop_limit=max_output_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
